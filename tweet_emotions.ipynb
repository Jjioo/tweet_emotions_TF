{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsjStGuxtEPp"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.callbacks import ProgbarLogger\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.callbacks import ProgbarLogger\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "GWfPyJflwIJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"tweet_emotions.csv\")"
      ],
      "metadata": {
        "id": "85AGB0kjFFjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reindex(columns=['tweet_id', 'content', 'sentiment'])"
      ],
      "metadata": {
        "id": "bj4Uc393yIkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,1].values.reshape(-1, 1)\n",
        "y = df.iloc[:,-1].values\n",
        "over = RandomOverSampler()\n",
        "X, y = over.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "f3DC4PWzxuD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Perform label encoding on y\n",
        "y_encoded = label_encoder.fit_transform(y) + 1\n",
        "\n",
        "# Reshape 'content' array to be 1-dimensional\n",
        "X_reshaped = X.reshape(-1)\n",
        "\n",
        "# Create a new DataFrame with 'sentiment' and 'content' columns\n",
        "df = pd.DataFrame({'content': X_reshaped,'sentiment': y_encoded})\n"
      ],
      "metadata": {
        "id": "ku-aeWyBx0bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Sc-HIP15x2oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = np.split(df.sample(frac=1), [int(0.8 * len(df)), int(0.9 * len(df))])"
      ],
      "metadata": {
        "id": "qtS8PbZQtkrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=64):\n",
        "    df = dataframe.copy()\n",
        "    labels = df.pop('sentiment')\n",
        "    df = df[\"content\"]\n",
        "\n",
        "    # Perform label encoding on labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "\n",
        "    # Convert labels to one-hot encoded format\n",
        "    labels_one_hot = tf.keras.utils.to_categorical(labels_encoded, num_classes=num_classes)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((df, labels_one_hot))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "mgCbnIgXyc6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding + Model"
      ],
      "metadata": {
        "id": "sEw18-Betoml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the embedding layer\n",
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, dtype=tf.string, trainable=True)\n",
        "\n",
        "train_data = df_to_dataset(train)\n",
        "val_data = df_to_dataset(val)\n",
        "test_data = df_to_dataset(test)"
      ],
      "metadata": {
        "id": "1p4ojda-thVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len([i for i in set(y)])"
      ],
      "metadata": {
        "id": "WkP5O49ZEujG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.CategoricalHinge(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation_data\n",
        "model.evaluate(val_data)\n"
      ],
      "metadata": {
        "id": "jbLV1aAltqkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, epochs=5, validation_data=val_data)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "rnkrSTlcEmdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#improve the model's performance:\n",
        "1- Increase the model's capacity\n",
        "\n",
        "2- Increase the model's capacity\n",
        "\n",
        "3- Use a different optimizer\n",
        "\n",
        "4- Increase the number of training epochs\n",
        "\n",
        "5- Perform data preprocessing\n",
        "\n",
        "6- Perform data preprocessing\n",
        "\n",
        "7- Perform data preprocessing\n"
      ],
      "metadata": {
        "id": "pssEEH4suSms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['content'])\n",
        "sequences = tokenizer.texts_to_sequences(df['content'])\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_length = 100  # adjust as needed\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert sentiment to one-hot encoded vectors\n",
        "sentiment_vectors = tf.keras.utils.to_categorical(df['sentiment'])\n",
        "\n",
        "# Create the input and output data for the model\n",
        "X = padded_sequences\n",
        "y = sentiment_vectors\n",
        "# Split the data into train and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "train_X, test_X = X[:train_size], X[train_size:]\n",
        "train_y, test_y = y[:train_size], y[train_size:]\n",
        "input_shape = X.shape[1]  # Assuming X is a NumPy array of shape (num_samples, num_features)\n",
        "vocab_size = len(np.unique(np.concatenate(X)))\n",
        "embedding_dim = 100  # Specify the dimensionality of the embedding\n",
        "max_length = max_length = max(len(sequence) for sequence in X)  # Specify the maximum length of your input sequences"
      ],
      "metadata": {
        "id": "T7VfMv463qsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = y.shape[1]  # Extract the number of classes from the target data"
      ],
      "metadata": {
        "id": "vwfcGBzm2WJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['content'])\n",
        "sequences = tokenizer.texts_to_sequences(df['content'])\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_length = 100  # adjust as needed\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert sentiment to one-hot encoded vectors\n",
        "sentiment_vectors = tf.keras.utils.to_categorical(df['sentiment'])\n",
        "\n",
        "# Create the input and output data for the model\n",
        "X = padded_sequences\n",
        "y = sentiment_vectors\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "train_X, test_X = X[:train_size], X[train_size:]\n",
        "train_y, test_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Specify the dimensionality of the embedding\n",
        "embedding_dim = 100\n",
        "\n",
        "# Specify the maximum length of your input sequences\n",
        "max_length = max(len(sequence) for sequence in X)\n",
        "\n",
        "# Specify the size of your vocabulary\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JdE0KD5q6FIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(train_X, train_y,batch_size=64, epochs=5,validation_data=(test_X, test_y), verbose=0, callbacks=[ProgbarLogger(count_mode='steps')])\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_X, test_y)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "abr3YSs06GS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eMHb_cP16wVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"tweet_emotions.csv\")"
      ],
      "metadata": {
        "id": "Xjec9F7i63JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['content'])\n",
        "sequences = tokenizer.texts_to_sequences(df['content'])"
      ],
      "metadata": {
        "id": "EdRqZEOd65DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify the dimensionality of the embedding\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "-BKPO4Y67U1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_X, test_y)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "feYEbDsK6xt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with RMSprop optimizer\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "test_loss, test_accuracy = model.evaluate(test_X, test_y)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "3HwpRV-D85Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, epochs=5, batch_size=16, validation_data=(test_X, test_y))"
      ],
      "metadata": {
        "id": "SMiMMhJM7ZlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_X, train_y, epochs=5, batch_size=16, validation_data=(test_X, test_y))"
      ],
      "metadata": {
        "id": "6I4nLWML-X64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the last accuracy value\n",
        "last_accuracy = round(history.history['accuracy'][-1], 2)\n",
        "print(\"Last Accuracy:\", last_accuracy)\n",
        "\n",
        "# Retrieve the last loss value\n",
        "last_loss = round(history.history['loss'][-1], 2)\n",
        "print(\"Last Loss:\", last_loss)\n",
        "\n",
        "# Retrieve the validation accuracy value\n",
        "val_accuracy = round(history.history['val_accuracy'][-1], 2)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n"
      ],
      "metadata": {
        "id": "p0_w6pLpQWQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}